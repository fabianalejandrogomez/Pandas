{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# used for dates\n",
    "import datetime\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Set some pandas options controlling output format\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 90)\n",
    "\n",
    "# bring in matplotlib for graphics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: data/msft.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# view the first five lines of data/msft.csv\n",
    "!head -n 5 data/msft.csv # mac or Linux\n",
    "# type data/msft.csv # on windows, but shows the entire file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a CSV into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/msft.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-535a6ad778b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in msft.csv into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmsft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/msft.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmsft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/msft.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# read in msft.csv into a DataFrame\n",
    "msft = pd.read_csv(\"data/msft.csv\")\n",
    "msft[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the index column when reading a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use column 0 as the index\n",
    "msft = pd.read_csv(\"data/msft.csv\", index_col=0)\n",
    "msft[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data type inference and specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the types of the columns in this DataFrame\n",
    "msft.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify that the Volume column should be a float64\n",
    "msft = pd.read_csv(\"data/msft.csv\", \n",
    "                   dtype = { 'Volume' : np.float64})\n",
    "msft.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a new set of names for the columns\n",
    "# all lower case, remove space in Adj Close\n",
    "# also, header=0 skips the header row\n",
    "df = pd.read_csv(\"data/msft.csv\", \n",
    "                 header=0,\n",
    "                 names=['date', 'open', 'high', 'low', \n",
    "                        'close', 'volume'])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying specific columns to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data only in the Date and Close columns\n",
    "# and index by the Date column\n",
    "df2 = pd.read_csv(\"data/msft.csv\", \n",
    "                  usecols=['Date', 'Close'], \n",
    "                  index_col=['Date'])\n",
    "df2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a DataFrame to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df2 to a new csv file\n",
    "# also specify naming the index as date\n",
    "df2.to_csv(\"data/msft_modified.csv\", index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the start of the file just saved\n",
    "!head -n 5 data/msft_modified.csv\n",
    "#type data/msft_modified.csv # windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General field-delimited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use read_table with sep=',' to read a CSV\n",
    "df = pd.read_table(\"data/msft.csv\", sep=',')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pipe delimited\n",
    "df.to_csv(\"data/msft_piped.txt\", sep='|')\n",
    "# check that it worked\n",
    "!head -n 5 data/msft_piped.txt # osx or Linux\n",
    "# type data/psft_piped.txt # on windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling variants of formats in field-delimited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messy file\n",
    "!head -n 6 data/msft2.csv # osx or Linux\n",
    "# type data/msft2.csv # windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read, but skip rows 0, 2 and 3\n",
    "df = pd.read_csv(\"data/msft2.csv\", skiprows=[0, 2, 3])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another messy file, with the mess at the end\n",
    "!cat data/msft_with_footer.csv # osx or Linux\n",
    "# type data/msft_with_footer.csv # windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip only two lines at the end\n",
    "df = pd.read_csv(\"data/msft_with_footer.csv\", \n",
    "                 skipfooter=2,\n",
    "                 engine = 'python')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only process the first three rows\n",
    "pd.read_csv(\"data/msft.csv\", nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip 100 lines, then only process the next five\n",
    "pd.read_csv(\"data/msft.csv\", skiprows=100, nrows=5, \n",
    "            header=0,\n",
    "            names=['date', 'open', 'high', 'low', \n",
    "                   'close', 'vol']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing data in Excel format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel file\n",
    "# only reads first sheet (msft in this case)\n",
    "df = pd.read_excel(\"data/stocks.xlsx\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the aapl worksheet\n",
    "aapl = pd.read_excel(\"data/stocks.xlsx\", sheetname='aapl')\n",
    "aapl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to an .XLS file, in worksheet 'Sheet1'\n",
    "df.to_excel(\"data/stocks2.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write making the worksheet name MSFT\n",
    "df.to_excel(\"data/stocks_msft.xls\", sheet_name='MSFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write multiple sheets\n",
    "# requires use of the ExcelWriter class\n",
    "from pandas import ExcelWriter\n",
    "with ExcelWriter(\"data/all_stocks.xls\") as writer:\n",
    "    aapl.to_excel(writer, sheet_name='AAPL')\n",
    "    df.to_excel(writer, sheet_name='MSFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to xlsx\n",
    "df.to_excel(\"data/msft2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wirite the excel data to a JSON file\n",
    "df[:5].to_json(\"data/stocks.json\")\n",
    "!cat data/stocks.json # osx or Linux\n",
    "#type data/stocks.json # windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in from JSON\n",
    "df_from_json = pd.read_json(\"data/stocks.json\")\n",
    "df_from_json[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the URL to read\n",
    "url = \"http://www.fdic.gov/bank/individual/failed/banklist.html\"\n",
    "# read it\n",
    "banks = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine a subset of the first table read\n",
    "banks[0][0:5].iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the stock data\n",
    "df = pd.read_excel(\"data/stocks.xlsx\")\n",
    "# write the first two rows to HTML\n",
    "df.head(2).to_html(\"data/stocks.html\")\n",
    "# check the first 28 lines of the output\n",
    "!head -n 10 data/stocks.html # max or Linux\n",
    "# type data/stocks.html # window, but prints the entire file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing HDF5 format files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for replication\n",
    "np.random.seed(123456)\n",
    "# create a DataFrame of dates and random numbers in three columns\n",
    "df = pd.DataFrame(np.random.randn(8, 3), \n",
    "                  index=pd.date_range('1/1/2000', periods=8),\n",
    "                  columns=['A', 'B', 'C'])\n",
    "\n",
    "# create HDF5 store\n",
    "store = pd.HDFStore('data/store.h5')\n",
    "store['df'] = df # persisting happened here\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from HDF5\n",
    "store = pd.HDFStore(\"data/store.h5\")\n",
    "df = store['df']\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this changes the DataFrame, but did not persist\n",
    "df.iloc[0].A = 1 \n",
    "# to persist the change, assign the DataFrame to the \n",
    "# HDF5 store object\n",
    "store['df'] = df\n",
    "# it is now persisted\n",
    "# the following loads the store and \n",
    "# shows the first two rows, demonstrating\n",
    "# the the persisting was done\n",
    "pd.HDFStore(\"data/store.h5\")['df'][:5] # it's now in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing data on the web and in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv directly from Yahoo! Finance from a URL\n",
    "msft_hist = pd.read_csv(\n",
    "    \"http://www.google.com/finance/historical?\" +\n",
    "    \"q=NASDAQ:MSFT&startdate=Apr+01%2C+2017&\" +\n",
    "    \"enddate=Apr+30%2C+2017&output=csv\")\n",
    "msft_hist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing from/to SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference SQLite\n",
    "import sqlite3\n",
    "\n",
    "# read in the stock data from CSV\n",
    "msft = pd.read_csv(\"data/msft.csv\")\n",
    "msft[\"Symbol\"]=\"MSFT\"\n",
    "aapl = pd.read_csv(\"data/aapl.csv\")\n",
    "aapl[\"Symbol\"]=\"AAPL\"\n",
    "\n",
    "# create connection\n",
    "connection = sqlite3.connect(\"data/stocks.sqlite\")\n",
    "# .to_sql() will create SQL to store the DataFrame\n",
    "# in the specified table.  if_exists specifies\n",
    "# what to do if the table already exists\n",
    "msft.to_sql(\"STOCK_DATA\", connection, if_exists=\"replace\")\n",
    "aapl.to_sql(\"STOCK_DATA\", connection, if_exists=\"append\")\n",
    "\n",
    "# commit the SQL and close the connection\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database file\n",
    "connection = sqlite3.connect(\"data/stocks.sqlite\")\n",
    "\n",
    "# query all records in STOCK_DATA\n",
    "# returns a DataFrame\n",
    "# inde_col specifies which column to make the DataFrame index\n",
    "stocks = pd.io.sql.read_sql(\"SELECT * FROM STOCK_DATA;\", \n",
    "                             connection, index_col='index')\n",
    "\n",
    "# close the connection\n",
    "connection.close()\n",
    "\n",
    "# report the head of the data retrieved\n",
    "stocks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the connection\n",
    "connection = sqlite3.connect(\"data/stocks.sqlite\")\n",
    "# construct the query string\n",
    "query = \"SELECT * FROM STOCK_DATA WHERE \" + \\\n",
    "        \"Volume>29200100 AND Symbol='MSFT';\"\n",
    "# execute and close connection\n",
    "items = pd.io.sql.read_sql(query, connection, index_col='index')\n",
    "connection.close()\n",
    "# report the query result\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading stock data from Google Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data reader package\n",
    "import pandas_datareader as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from google and display the head of the data\n",
    "start = datetime(2017, 4, 1)\n",
    "end = datetime(2017, 4, 30)\n",
    "goog = pdr.data.DataReader(\"MSFT\", 'google', start, end)\n",
    "goog[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving options data from Google Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read options for MSFT\n",
    "options = pdr.data.Options('MSFT', 'google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.expiry_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = options.get_options_data(expiry=options.expiry_dates[0])\n",
    "data.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all puts at strike price of $30 (first four columns only)\n",
    "data.loc[(30, slice(None), 'put'), :].iloc[0:5, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put options at strike of $80, between 2017-06-01 and 2017-06-30\n",
    "data.loc[(30, slice('20180119','20180130'), 'put'), :] \\\n",
    "    .iloc[:, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading economic data from the Federal Reserve Bank of St. Louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read GDP data from FRED\n",
    "gdp = pdr.data.FredReader(\"GDP\",\n",
    "                     date(2012, 1, 1), \n",
    "                     date(2014, 1, 27))\n",
    "gdp.read()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Compensation of employees: Wages and salaries\n",
    "pdr.data.FredReader(\"A576RC1A027NBEA\",\n",
    "                date(1929, 1, 1),\n",
    "                date(2013, 1, 1)).read()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Kenneth French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from Kenneth French fama global factors data set\n",
    "factors = pdr.data.FamaFrenchReader(\"Global_Factors\").read()\n",
    "factors[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from the World Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all indicators\n",
    "from pandas_datareader import wb\n",
    "all_indicators = pdr.wb.get_indicators()\n",
    "all_indicators.iloc[:5,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search of life expectancy indicators\n",
    "le_indicators = pdr.wb.search(\"life expectancy\")\n",
    "# report first three rows, first two columns\n",
    "le_indicators.iloc[:5,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get countries and show the 3 digit code and name\n",
    "countries = pdr.wb.get_countries()\n",
    "# show a subset of the country data\n",
    "countries.loc[0:5,['name', 'capitalCity', 'iso2c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get life expectancy at birth for all countries from 1980 to 2014\n",
    "le_data_all = pdr.wb.download(indicator=\"SP.DYN.LE00.IN\", \n",
    "                          start='1980', \n",
    "                          end='2014')\n",
    "le_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only US, CAN, and MEX are returned by default\n",
    "le_data_all.index.levels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve life expectancy at birth for all countries \n",
    "# from 1980 to 2014\n",
    "le_data_all = wb.download(indicator=\"SP.DYN.LE00.IN\", \n",
    "                          country = countries['iso2c'],\n",
    "                          start='1980', \n",
    "                          end='2012')\n",
    "le_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le_data_all.pivot(index='country', columns='year')\n",
    "le_data = le_data_all.reset_index().pivot(index='country', \n",
    "                                          columns='year')\n",
    "# examine pivoted data\n",
    "le_data.iloc[:5,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask what is the name of country for each year\n",
    "# with the least life expectancy\n",
    "country_with_least_expectancy = le_data.idxmin(axis=0)\n",
    "country_with_least_expectancy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and what is the minimum life expectancy for each year\n",
    "expectancy_for_least_country = le_data.min(axis=0)\n",
    "expectancy_for_least_country[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this merges the two frames together and gives us\n",
    "# year, country and expectancy where there minimum exists\n",
    "least = pd.DataFrame(\n",
    "    data = {'Country': country_with_least_expectancy.values,\n",
    "            'Expectancy': expectancy_for_least_country.values},\n",
    "    index = country_with_least_expectancy.index.levels[1])\n",
    "least[:5]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
